{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Logistic Regression\n",
    "## CSCI 5622 - Spring 2019\n",
    "***\n",
    "**Name**: $<$Pranav Gummaraj Srinivas$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11.59 PM on Tuesday, February 12th**. Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your classmates and instructors, but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "Your task for this homework is to build a logistic regression model that implements stochastic gradient ascent. To start, you'll apply it to the task of determining whether a number is 8 or 9\n",
    "\n",
    "We start by importing and plotting the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [ 50 points] Problem 1: Implementing the Logistic Regression Classifier\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import pickle, gzip       \n",
    "import numpy as np\n",
    "\n",
    "class Numbers:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 9 and 8 only\n",
    "    \"\"\" \n",
    "    def __init__(self, location):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    " \n",
    "        self.train_x, self.train_y = train_set\n",
    "        train_indices = np.where(self.train_y > 7)\n",
    "        self.train_x, self.train_y = self.train_x[train_indices], self.train_y[train_indices]\n",
    "        self.train_y = self.train_y - 8\n",
    " \n",
    "        self.valid_x, self.valid_y = valid_set\n",
    "        valid_indices = np.where(self.valid_y > 7)\n",
    "        self.valid_x, self.valid_y = self.valid_x[valid_indices], self.valid_y[valid_indices]\n",
    "        self.valid_y = self.valid_y - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you'll implement a Logistic Regression classifier to take drawings of either an eight or a nine and output the corresponding label.\n",
    "1.1 - Finish the `calculate_score` function to return the output of applying the dot product of the weights with the input parameter\n",
    "\n",
    "1.2 - Finish the `sigmoid` function to return the output of applying the sigmoid function to the calculated score\n",
    "\n",
    "1.3 - Finish the `compute_gradient` function to return the derivate of the cost w.r.t. the weights\n",
    "\n",
    "1.4 - Finish the `sgd_update` function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly\n",
    "\n",
    "1.5 - Finish the `mini_batch_update` function so that it performs mini-batch gradient descent on the batches of the training data set example and updates the weight vector correspondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "class LogReg:\n",
    "    \n",
    "    def __init__(self, X, y, eta = 0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: Learning rate (the default is a constant value)\n",
    "        :method: This should be the name of the method (sgd_update or mini_batch_descent)\n",
    "        :batch_size: optional argument that is needed only in the case of mini_batch_descent\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #self.w = np.zeroes(X.shape[1]+1) # can remove from here and ask to be defined in the function\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.eta = eta\n",
    "        \n",
    "    def calculate_score(self, x):\n",
    "        \"\"\"\n",
    "        :param x: This can be a single training example or it could be n training examples\n",
    "        :return score: Calculate the score that you will plug into the logistic function\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) != 1:\n",
    "            self.score = []\n",
    "            for each_x in x:\n",
    "                self.each_score = np.dot(self.w, each_x.T)\n",
    "                self.score.append(self.each_score)\n",
    "        else:\n",
    "            self.score = np.dot(self.w, x.T)\n",
    "        return self.score\n",
    "    \n",
    "    def sigmoid(self, score):\n",
    "        \"\"\"\n",
    "        :param score: Either a real valued number or a vector to convert into a number between 0 and 1\n",
    "        :return sigmoid: Calcuate the output of applying the sigmoid function to the score. This could be a single\n",
    "        value or a vector depending on the input\n",
    "        \"\"\"\n",
    "        if isinstance(score, list):\n",
    "            self.output = []\n",
    "            for each_score in score:\n",
    "                answer = 1 / (1 + np.exp(-each_score))\n",
    "                self.output.append(answer)\n",
    "        else:\n",
    "            self.output = 1 / (1 + np.exp(-score))\n",
    "        return self.output\n",
    "    \n",
    "    def compute_gradient(self, x, h, y):\n",
    "        \"\"\"\n",
    "        :param x: Feature vector\n",
    "        :param h: predicted class label\n",
    "        :param y: real class label\n",
    "        :return gradient: Return the derivate of the cost w.r.t to the weights\n",
    "        \"\"\"\n",
    "        gradient = []\n",
    "        for k in range(len(self.w)):\n",
    "            gradient.append(0)\n",
    "            gradient[k] = (h - y)*x[k]\n",
    "        return gradient\n",
    "        \n",
    "     \n",
    "    def sgd_update(self):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update over the entire dataset to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\" \n",
    "        # TODO: Finish this function to do a stochastic gradient descent update over the entire dataset\n",
    "        # and return the updated weight vector\n",
    "        #self.x = np.insert(self.X, 0, 1, axis=1)\n",
    "        #self.x, self.y = shuffle(self.x, self.y)\n",
    "        for i in range(len(self.X)):\n",
    "            #self.h = self.sigmoid(self.calculate_score(np.reshape(self.X[i], (-1, len(self.X[i])))))\n",
    "            self.h = self.sigmoid(self.calculate_score(self.X[i]))\n",
    "            for k in range(len(self.X[0])):\n",
    "                self.w[k] = self.w[k] - (self.eta*self.compute_gradient(self.X[i], self.h, self.y[i])[k])\n",
    "        return self.w\n",
    "        \n",
    "    \n",
    "    def mini_batch_update(self, batch_size):\n",
    "        \"\"\"\n",
    "        One iteration of the mini-batch update over the entire dataset (one sweep of the dataset).\n",
    "        :param X: NumPy array of features (size : no of examples X features)\n",
    "        :param y: Numpy array of class labels (size : no of examples X 1)\n",
    "        :param batch_size: size of the batch for gradient update\n",
    "        :returns w: Coefficients of the classifier (after updating)\n",
    "        \"\"\"\n",
    "        # TODO: Performing mini-batch training follows the same steps as in stochastic gradient descent,\n",
    "        # the only major difference is that we’ll use batches of training examples instead of one. \n",
    "        # Here we decide a batch size, which is the number of examples that will be fed into the \n",
    "        # computational graph at once\n",
    "        i = 0\n",
    "        j = 1\n",
    "        self.h = self.sigmoid(self.calculate_score(self.X))\n",
    "        self.gradient = [0] * len(self.X[0])\n",
    "        for x in self.X:\n",
    "            if i < (batch_size*j):\n",
    "                self.temp_gradient = self.compute_gradient(self.X[i], self.h[i], self.y[i])\n",
    "                for k in range(len(self.X[0])):\n",
    "                    self.gradient[k] += self.temp_gradient[k]\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "                for k in range(len(self.X[0])):\n",
    "                    self.w[k] = self.w[k] - self.eta*self.gradient[k]\n",
    "                self.h = self.sigmoid(self.calculate_score(self.X))\n",
    "        else:\n",
    "            for k in range(len(self.X[0])):\n",
    "                self.w[k] = self.w[k] - self.eta*self.gradient[k]\n",
    "        return self.w\n",
    "    \n",
    "    def progress(self, test_x, test_y, update_method, *batch_size):\n",
    "        \"\"\"\n",
    "        Given a set of examples, computes the probability and accuracy\n",
    "        :param test_x: The features of the test dataset to score\n",
    "        :param test_y: The features of the test \n",
    "        :param update_method: The update method to be used, either 'sgd_update' or 'mini_batch_update'\n",
    "        :param batch_size: Optional arguement to be given only in case of mini_batch_update\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "        # TODO: Complete this function to compute the predicted value for an example based on the logistic value\n",
    "        # and return the log probability and the accuracy of those predictions\n",
    "        self.predictions = []\n",
    "        if update_method == \"sgd_update\":\n",
    "            self.weights = self.sgd_update()\n",
    "        if update_method == \"mini_batch_update\":\n",
    "            self.weights = self.mini_batch_update(batch_size[0])\n",
    "        for x in test_x:\n",
    "            self.h = self.sigmoid(self.calculate_score(x))\n",
    "            if self.h >= 0.5:\n",
    "                self.predictions.append(1)\n",
    "            else:\n",
    "                self.predictions.append(0)\n",
    "        self.correct_predictions = 0\n",
    "        self.log_probability = 0\n",
    "        for i in range(len(test_y)):\n",
    "            if test_y[i] == self.predictions[i]:\n",
    "                correct_predictions += 1\n",
    "            if test_y[i] == 1:\n",
    "                self.log_probability += math.log(self.sigmoid(self.calculate_score(test_x[i])))\n",
    "            else:\n",
    "                self.log_probability += math.log(1-self.sigmoid(self.calculate_score(test_x[i])))\n",
    "        self.accuracy = self.correct_predictions/len(test_y)\n",
    "        \n",
    "        return self.log_probability, self.accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg1:\n",
    "    \n",
    "    def __init__(self, X, y, eta = 0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: Learning rate (the default is a constant value)\n",
    "        :method: This should be the name of the method (sgd_update or mini_batch_descent)\n",
    "        :batch_size: optional argument that is needed only in the case of mini_batch_descent\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #self.w = np.zeroes(X.shape[1]+1) # can remove from here and ask to be defined in the function\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.eta = eta\n",
    "        \n",
    "    def calculate_score(self, x):\n",
    "        \"\"\"\n",
    "        :param x: This can be a single training example or it could be n training examples\n",
    "        :return score: Calculate the score that you will plug into the logistic function\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) != 1:\n",
    "            self.score = []\n",
    "            for each_x in x:\n",
    "                self.each_score = np.dot(self.w, each_x.T)\n",
    "                self.score.append(self.each_score)\n",
    "        else:\n",
    "            self.score = np.dot(self.w, x.T)\n",
    "        return self.score\n",
    "    \n",
    "    def sigmoid(self, score):\n",
    "        \"\"\"\n",
    "        :param score: Either a real valued number or a vector to convert into a number between 0 and 1\n",
    "        :return sigmoid: Calcuate the output of applying the sigmoid function to the score. This could be a single\n",
    "        value or a vector depending on the input\n",
    "        \"\"\"\n",
    "        if isinstance(score, list):\n",
    "            self.output = []\n",
    "            for each_score in score:\n",
    "                answer = 1 / (1 + np.exp(-each_score))\n",
    "                self.output.append(answer)\n",
    "        else:\n",
    "            #print(score)\n",
    "            self.output = 1 / (1 + np.exp(-score))\n",
    "        return self.output\n",
    "    \n",
    "    def compute_gradient(self, x, h, y):\n",
    "        \"\"\"\n",
    "        :param x: Feature vector\n",
    "        :param h: predicted class label\n",
    "        :param y: real class label\n",
    "        :return gradient: Return the derivate of the cost w.r.t to the weights\n",
    "        \"\"\"\n",
    "        gradient = []\n",
    "        for k in range(len(self.w)):\n",
    "            gradient.append(0)\n",
    "            gradient[k] = (h - y)*x[k]\n",
    "        #print(f\"Gradient is {gradient}\")\n",
    "        return gradient\n",
    "        \n",
    "     \n",
    "    def sgd_update(self):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update over the entire dataset to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\" \n",
    "        # TODO: Finish this function to do a stochastic gradient descent update over the entire dataset\n",
    "        # and return the updated weight vector\n",
    "        #self.x = np.insert(self.X, 0, 1, axis=1)\n",
    "        #self.x, self.y = shuffle(self.x, self.y)\n",
    "        print(f\"Shape of X {np.shape(self.X)}\")\n",
    "        for i in range(len(self.X)):\n",
    "            #print(f\"{i}th element of X\")\n",
    "            #self.h = self.sigmoid(self.calculate_score(np.reshape(self.X[i], (-1, len(self.X[i])))))\n",
    "            self.h = self.sigmoid(self.calculate_score(self.X[i]))\n",
    "            self.gradient = self.compute_gradient(self.X[i], self.h, self.y[i])\n",
    "            for k in range(len(self.X[0])):\n",
    "                self.w[k] = self.w[k] - (self.eta*self.gradient[k])\n",
    "        return self.w\n",
    "    \n",
    "    def mini_batch_update(self, batch_size):\n",
    "        \"\"\"\n",
    "        One iteration of the mini-batch update over the entire dataset (one sweep of the dataset).\n",
    "        :param X: NumPy array of features (size : no of examples X features)\n",
    "        :param y: Numpy array of class labels (size : no of examples X 1)\n",
    "        :param batch_size: size of the batch for gradient update\n",
    "        :returns w: Coefficients of the classifier (after updating)\n",
    "        \"\"\"\n",
    "        # TODO: Performing mini-batch training follows the same steps as in stochastic gradient descent,\n",
    "        # the only major difference is that we’ll use batches of training examples instead of one. \n",
    "        # Here we decide a batch size, which is the number of examples that will be fed into the \n",
    "        # computational graph at once\n",
    "        i = 0\n",
    "        j = 1\n",
    "        #self.h = self.sigmoid(self.calculate_score(self.X))\n",
    "        print(\"bathc update\")\n",
    "        self.gradient = [0] * len(self.X[0])\n",
    "        for x in self.X:\n",
    "            if i < (batch_size*j):\n",
    "                #print(self.w)\n",
    "                self.h = self.sigmoid(self.calculate_score(self.X[i]))\n",
    "                self.temp_gradient = self.compute_gradient(self.X[i], self.h, self.y[i])\n",
    "                for k in range(len(self.X[0])):\n",
    "                    self.gradient[k] += self.temp_gradient[k]\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "                for k in range(len(self.X[0])):\n",
    "                    self.w[k] = self.w[k] - self.eta*self.gradient[k]\n",
    "        else:\n",
    "            for k in range(len(self.X[0])):\n",
    "                self.w[k] = self.w[k] - self.eta*self.gradient[k]\n",
    "        return self.w\n",
    "    \n",
    "    def progress(self, test_x, test_y, update_method, epochs, *batch_size):\n",
    "        print(\"progress\")\n",
    "        \"\"\"\n",
    "        Given a set of examples, computes the probability and accuracy\n",
    "        :param test_x: The features of the test dataset to score\n",
    "        :param test_y: The features of the test\n",
    "        :param update_method: The update method to be used, either 'sgd_update' or 'mini_batch_update'\n",
    "        :param batch_size: Optional arguement to be given only in case of mini_batch_update\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "        # TODO: Complete this function to compute the predicted value for an example based on the logistic value\n",
    "        # and return the log probability and the accuracy of those predictions\n",
    "        self.accuracy_list = []\n",
    "        if update_method == \"sgd_update\":\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print(f\"epoch {epoch}\")\n",
    "                self.correct_predictions = 0\n",
    "                self.weights = self.sgd_update()\n",
    "                #print(f\"weigths = {self.weights}\")\n",
    "                for i in range(len(test_x)):\n",
    "                    self.h = self.sigmoid(self.calculate_score(test_x[i]))\n",
    "                    if self.h >= 0.5:\n",
    "                        if test_y[i] == 1:\n",
    "                            self.correct_predictions += 1\n",
    "                    else:\n",
    "                        if test_y[i] == 0:\n",
    "                            self.correct_predictions += 1\n",
    "                self.accuracy = self.correct_predictions / len(test_y)\n",
    "                self.accuracy_list.append(self.accuracy)\n",
    "                self.X, self.y = shuffle(self.X, self.y)\n",
    "                #print(self.accuracy_list)\n",
    "        if update_method == \"mini_batch_update\":\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print(f\"epoch {epoch}\")\n",
    "                self.correct_predictions = 0\n",
    "                self.weights = self.mini_batch_update(batch_size[0])\n",
    "                for i in range(len(test_x)):\n",
    "                    self.h = self.sigmoid(self.calculate_score(test_x[i]))\n",
    "                    if self.h >= 0.5:\n",
    "                        if test_y[i] == 1:\n",
    "                            self.correct_predictions += 1\n",
    "                    else:\n",
    "                        if test_y[i] == 0:\n",
    "                            self.correct_predictions += 1\n",
    "\n",
    "                self.accuracy = self.correct_predictions / len(test_y)\n",
    "                self.accuracy_list.append(self.accuracy)\n",
    "                self.X, self.y = shuffle(self.X, self.y)\n",
    "                #print(self.accuracy_list)\n",
    "        return self.accuracy_list\n",
    "    \n",
    "    def calculate_h(self, test_x, test_y):\n",
    "        # TODO: Complete this function to compute the predicted value for an example based on the logistic value\n",
    "        # and return the log probability and the accuracy of those predictions\n",
    "        self.accuracy_list = []\n",
    "        self.correct_predictions = 0\n",
    "        for i in range(len(test_x)):\n",
    "            self.h = self.sigmoid(self.calculate_score(test_x[i]))\n",
    "            \"\"\"\n",
    "                if self.h >= 0.5:\n",
    "                    if test_y[i] == 1:\n",
    "                        self.correct_predictions += 1\n",
    "                else:\n",
    "                    if test_y[i] == 0:\n",
    "                        self.correct_predictions += 1\n",
    "\n",
    "            self.accuracy = self.correct_predictions / len(test_y)\n",
    "            self.accuracy_list.append(self.accuracy)\n",
    "            \"\"\"\n",
    "                #print(self.accuracy_list)\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class LogRegTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.X = np.array([[0.1, 0.3 ], [0.4, 0.6], [0.8, 0.1], [0.8, 0.1], [0.5, 0.8]])\n",
    "        self.y = np.array([0,  0, 1, 1,  0])\n",
    "        self.log_reg_classifier_1 = LogReg(self.X, self.y, 0.5)\n",
    "        self.log_reg_classifier_2 = LogReg(self.X, self.y, 0.5)\n",
    "  \n",
    "    def test_sgd_update(self):\n",
    "        \n",
    "        #Test sgd_update function from LogReg\n",
    "        \n",
    "        weights = self.log_reg_classifier_1.sgd_update()\n",
    "        self.assertEqual(round(weights[0], 2), 0.16)\n",
    "        self.assertEqual(round(weights[1], 2), -0.37)\n",
    "       \n",
    "    def tests_mini_batch_update(self):\n",
    "        \n",
    "        #Test mini_batch_update function from LogReg\n",
    "        \n",
    "        weights = self.log_reg_classifier_2.mini_batch_update(2)\n",
    "        self.assertEqual(round(weights[0], 2), 0.17)\n",
    "        self.assertEqual(round(weights[1], 2), -0.4)\n",
    "\n",
    "    \n",
    "    def tests_progress_sgd_update(self):\n",
    "        \"\"\"\n",
    "        Test progress function from LogReg with method = 'sgd_update'\n",
    "        \"\"\"\n",
    "        self.log_reg_classifier_1 = LogReg(self.X[:4], self.y[:4], 0.5)\n",
    "        log_prob, accuracy = self.log_reg_classifier_1.progress(self.X[4:], self.y[4:], 'sgd_update')\n",
    "        self.assertEqual(round(log_prob, 2), -0.7)\n",
    "        self.assertEqual(accuracy, 0)   \n",
    "    \n",
    "        \n",
    "    \n",
    "    #BEGIN Workspace\n",
    "    #Add more test functions as required\n",
    "    #END Workspace\n",
    "    \n",
    "tests = LogRegTester()\n",
    "myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [20 Points] Problem 2: Understanding the limits of the Logistic Regression Classifier\n",
    "\n",
    "2.1 - After completing the class above, loop over the training data and perform ___stochastic gradient descent___ for three different user-defined number of epochs 1, 3, 10], and five different values of eta range [.0001, .01, .1, .5, 1]. Train your model and do the following:\n",
    "\n",
    "* Using the `progress` method, calculate the accuracy on the training and the valid sets every 100 iterations. Plot them on same graph for every comparison.\n",
    "\n",
    "* Using `progress` method, calculate the accuracy on the validation set and store it for every epoch.\n",
    "\n",
    "Don't forget to shuffle your training data after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotGraph:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = int(epochs)\n",
    "    \"\"\"    \n",
    "    def setup(self):\n",
    "        num = Numbers('data/mnist.pklz')\n",
    "        self.train_x = num.train_x\n",
    "        self.train_y = num.train_y\n",
    "        self.valid_x = num.valid_x\n",
    "        self.valid_y = num.valid_y\n",
    "    \"\"\"\n",
    "    def sgd_graph(self):\n",
    "        self.num = Numbers('data/mnist.pklz')\n",
    "        self.eta_range = [.0001, .01, .1, .5, 1]\n",
    "        #self.eta_range = [.5, 1]\n",
    "        for eta in self.eta_range:\n",
    "            classifier = LogReg1(self.num.train_x, self.num.train_y, eta)\n",
    "            self.accuracy_list = classifier.progress(self.num.valid_x, self.num.valid_y, 'sgd_update', self.epochs)\n",
    "            plt.plot(range(1, self.epochs+1), self.accuracy_list, label=\"ETA_\"+str(eta)+\"_test\")\n",
    "            self.accuracy_list_train = classifier.progress(self.num.train_x, self.num.train_y, 'sgd_update', self.epochs)\n",
    "            plt.plot(range(1, self.epochs+1), self.accuracy_list, label=\"ETA_\"+str(eta)+\"_train\")\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.title(\"SGD accuracy plots\")\n",
    "        plt.show()\n",
    "    def mini_batch_graph(self):\n",
    "        self.num = Numbers('data/mnist.pklz')\n",
    "        self.eta_range = [.0001, .01, .1, .5, 1]\n",
    "        for eta in self.eta_range:\n",
    "            classifier = LogReg1(self.num.train_x, self.num.train_y, eta)\n",
    "            self.accuracy_list = classifier.progress(self.num.valid_x, self.num.valid_y, 'mini_batch_update', self.epochs, 500)\n",
    "            plt.plot(range(1, self.epochs+1), self.accuracy_list, label=\"ETA_\"+str(eta)+\"_test\")\n",
    "            self.accuracy_list = classifier.progress(self.num.valid_x, self.num.valid_y, 'mini_batch_update', self.epochs, 500)\n",
    "            plt.plot(range(1, self.epochs+1), self.accuracy_list, label=\"ETA_\"+str(eta)+\"_train\")\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.title(\"Mini-batch-update accuracy plots\")\n",
    "        plt.show()\n",
    "\n",
    "#graph = PlotGraph(10)\n",
    "#graph.sgd_graph()\n",
    "#graph.mini_batch_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 - After completing the class above, loop over the training data and perform ___mini batch gradient descent___ for three different user-defined number of epochs 1, 3, 10], and five different values of eta range [.0001, .01, .1, .5, 1]. Train your model and do the following:\n",
    "\n",
    "* Using the `progress` method, calculate the accuracy on the training and the valid sets every 100 iterations. Plot them on same graph for every comparison.\n",
    "\n",
    "* Using `progress` method, calculate the accuracy on the validation set and store it for every epoch.\n",
    "\n",
    "Don't forget to shuffle your training data after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Briefly describe the role of learning rate (eta) on the efficiency of convergence during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Briefly describe the role of the number of epochs on validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ 30 points] Problem 4: Implementing the Logistic Regression Classifier for Multinomial Classification\n",
    "\n",
    "You will now create a classifier that is commonly referred to as Multinomial Logistic Regression. The particular method you will be implementing is **One Vs All** or **One Vs Rest**. The dataset will be the MNIST dataset which includes all digits 0-9. You are free to use the functions you created above as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numbers2:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 0-9\n",
    "    \"\"\" \n",
    "    def __init__(self, location):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    " \n",
    "        self.train_x, self.train_y = train_set\n",
    "        self.test_x, self.test_y = valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "true label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = Numbers2('data/mnist.pklz')\n",
    "print(data2.train_y[:10])\n",
    "def view_digit(example, label=None):\n",
    "    if label is not None: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(28,28), cmap='gray');\n",
    "view_digit(data2.train_x[18],data2.train_y[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot\n",
      "get_optimal_parameters\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "bathc update\n",
      "predict\n",
      "[[0, 1.0], [1, 0.0], [2, 0.0], [3, 0.0], [4, 0.0], [5, 0.0], [6, 0.0], [7, 0.0], [8, 0.0], [9, 0.0]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class MultiLogReg:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 0-9\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, X, y, eta = 0.1):\n",
    "        #self.X = self.normalize_data(X)\n",
    "        self.X = X\n",
    "        self.y = self.one_hot_encoding(y)\n",
    "        self.eta = eta\n",
    "        self.reg_model = self.get_optimal_parameters() \n",
    "    def one_hot_encoding(self, y):\n",
    "        print('one_hot')\n",
    "        # TO DO: Represent the output vector y as a one hot encoding. Create a matrix of dimensions (m X 10) \n",
    "        # where m = number of examples, and 10 for number of classes\n",
    "        # if the class for the ith example is 7, then y[i][7] = 1 and the for k != 7, y[i][k] = 0.\n",
    "        self.encod = np.zeros((len(y), 10), dtype=int)\n",
    "        for i in range(len(y)):\n",
    "            self.encod[i][y[i]] = 1\n",
    "    \"\"\"\n",
    "    def normalize_data(self, X):\n",
    "        # TO DO: Normalize the dataset X using the mean and standard deviation of all the training examples \n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = std = np.std(X, axis=0)\n",
    "        for i in range(len(X[0])):\n",
    "            #print(len(X[0]))\n",
    "            #print(i)\n",
    "            if std[i] != 0.0:\n",
    "                for j in range(len(X)):\n",
    "                    #print(std)\n",
    "                    X[j][i] = (X[j][i] - mean[i])/std[i]\n",
    "                    #print(X[j][i])\n",
    "    \"\"\"\n",
    "    def get_optimal_parameters(self):\n",
    "        print('get_optimal_parameters')\n",
    "        # TO DO: This is the main training loop. You will have to find the optimal weights for all 10 models\n",
    "        # Each model is fit to its class which is (0-9), and the cost function will be against all of the other \n",
    "        # numbers, i.e. \"the rest\".\n",
    "        self.reg = []\n",
    "        for i in range(10):\n",
    "            self.reg.append(LogReg1(self.X, self.encod.T[i], self.eta))\n",
    "            self.reg[i].mini_batch_update(200)\n",
    "        return self.reg\n",
    "    \n",
    "    def predict(self, test_image, test_label):\n",
    "        print('predict')\n",
    "        # TO DO: This function should return the probabilities predicted by each of the models for some given \n",
    "        # input image. The probabilities are sorted with the most likely being listed first.\n",
    "        # Return a vector of shape (10, 2) with the first column holding the number and the second column with\n",
    "        # the probability that the test_image is that number\n",
    "        #self.reg_model = self.get_optimal_parameters()\n",
    "        #if len(np.shape(test_image)) == 1:\n",
    "        self.test_image = np.reshape(test_image, (-1, len(test_image)))\n",
    "        self.test_label = np.array([test_label])\n",
    "        #self.test_image = self.normalize_data(self.test_image)\n",
    "        self.probabilities = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            prblty = self.reg_model[i].calculate_h(self.test_image, self.test_label)\n",
    "            self.probabilities.append([i, prblty])\n",
    "        return self.probabilities\n",
    "    \n",
    "\n",
    "multi_classifier = MultiLogReg(data2.train_x, data2.train_y, 0.5)\n",
    "print(multi_classifier.predict(data2.test_x[208], data2.test_y[208]))\n",
    "print(data2.test_y[208])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([3,5,1,4,5])\n",
    "encod = np.zeros((len(y), 10), dtype=int)\n",
    "for i in range(len(y)):\n",
    "    encod[i][y[i]] = 1\n",
    "#print(encod.T)\n",
    "\n",
    "z = np.array([1,2,3,4])\n",
    "v = np.reshape(z, (-1, 4))\n",
    "print(len(np.shape(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "class test:\n",
    "    x = 0\n",
    "    def te(self):\n",
    "        x = 1\n",
    "    def pr(self):\n",
    "        print(self.x)\n",
    "a = test()\n",
    "a.pr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION ###\n",
    "It is important to know how well your model did on the whole. You need to report the ___accuracy as a percentage___ on the training set and the test set from Numbers2. You should also plot a ___confusion matrix___ for both, just like you did on the last homework and mention the numbers that were misclassified the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: COMPUTE ACCURACY ON THE TRAIN AND TEST DATA FROM NUMBERS2\n",
    "class Accuracy:\n",
    "    def __init__(self, train_x, train_y, eta = 0.01):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.eta = eta\n",
    "    \n",
    "    def get_predictions(self, test_x, test_y):\n",
    "        self.multi_classifier = MultiLogReg(self.train_x, self.train_y, self.eta)\n",
    "        self.predictions = []\n",
    "        self.mx = 0\n",
    "        self.high = 0\n",
    "        for i in range(len(test_x)):\n",
    "            self.predict = self.multi_classifier.predict(test_x[i], test_y[i])\n",
    "            for element in self.predict:\n",
    "                if element[1] > self.high:\n",
    "                    self.high = element[1]\n",
    "                    self.mx = element[0]\n",
    "            self.predictions.append(self.mx)\n",
    "        return self.predictions\n",
    "    \n",
    "    def get_accuracy(self, test_x, test_y):\n",
    "        self.predictions = self.get_predictions(test_x, test_y)\n",
    "        correct = 0\n",
    "        for i in range(len(self.predictions)):\n",
    "            if self.predictions[i] == y[i]:\n",
    "                correct += 1\n",
    "        self.accuracy = correct/len(y)\n",
    "        return self.accuracy\n",
    "    \n",
    "#evaluate = Accuracy(data2.train_x, data2.train_y, 0.5)\n",
    "#test_accuracy = evaluate.get_accuracy(data2.test_x, data2.test_y)\n",
    "#train_accuracy = evaluate.get_accuracy(data2.train_x, data2.train_y)\n",
    "#print(test_accuracy)\n",
    "#print(train_accuracy)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: PLOT THE CONFUSION MATRIX ON TEST AND TRAIN DATA. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
